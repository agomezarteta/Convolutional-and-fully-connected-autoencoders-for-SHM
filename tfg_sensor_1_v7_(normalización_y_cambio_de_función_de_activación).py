# -*- coding: utf-8 -*-
"""TFG SENSOR 1 v7 (normalización y cambio de función de activación)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IFyLFlrVG8L6T0b5akImlrEiPs8zOIga

#AUTOENCODER PARA DETECTAR ANOMALÍAS EN ESTRUCTURAS

## LIBRERÍAS
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers
from matplotlib import pyplot as plt
from tensorflow.keras.layers import Conv1D, Conv1DTranspose, Dense, Flatten, Reshape, Dropout
from tensorflow.keras.models import Sequential

from sklearn.metrics import mean_squared_error
def rmse(y,yprime):
    return np.sqrt(mean_squared_error(y,yprime))

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

! ls
# %cd /content/drive/MyDrive/Colab\ Notebooks/Sensor1
! ls
! pwd

"""##**SENSOR 1**

"""

sensor1_s = pd.read_excel("PROBETA1_BASELINE_sensor1_AUTOENCODER.xlsx")
print(sensor1_s.head())

min_reference_b = sensor1_s.min().min()
max_reference_b = sensor1_s.max().max()
min = min_reference_b
max = max_reference_b
print(max)
print(min)

"""

## DETERMINACIÓN DE LA REFERRENCIA PARA LA NORMALIZACIÓN DE DATOS
"""

testS1_pc1_s = pd.read_excel("PROBETA1_PASO_CARGA_1_sensor1_AUTOENCODER.xlsx")
min_reference_1 = testS1_pc1_s.min().min()
max_reference_1 = testS1_pc1_s.max().max()
print(max_reference_1)
print(min_reference_1)
if max_reference_1 > max :
  max = max_reference_1
  min = min_reference_1
print(max)
print(min)

testS1_pc2_s = pd.read_excel("PROBETA1_PASO_CARGA_2_sensor1_AUTOENCODER.xlsx")
min_reference_2 = testS1_pc2_s.min().min()
max_reference_2 = testS1_pc2_s.max().max()
print(max_reference_2)
print(min_reference_2)
if max_reference_2 > max :
  max = max_reference_2
  min = min_reference_2
print(max)
print(min)

testS1_pc3_s = pd.read_excel("PROBETA1_PASO_CARGA_3_sensor1_AUTOENCODER.xlsx")
min_reference_3 = testS1_pc3_s.min().min()
max_reference_3 = testS1_pc3_s.max().max()
print(max_reference_3)
print(min_reference_3)
if max_reference_3 > max :
  max = max_reference_3
  min = min_reference_3
print(max)
print(min)

testS1_pc4_s = pd.read_excel("PROBETA1_PASO_CARGA_4_sensor1_AUTOENCODER.xlsx")
min_reference_4 = testS1_pc4_s.min().min()
max_reference_4 = testS1_pc4_s.max().max()
print(max_reference_4)
print(min_reference_4)
if max_reference_4 > max :
  max = max_reference_4
  min = min_reference_4
print(max)
print(min)

testS1_pc5_s = pd.read_excel("PROBETA1_PASO_CARGA_5_sensor1_AUTOENCODER.xlsx")
min_reference_5 = testS1_pc5_s.min().min()
max_reference_5 = testS1_pc5_s.max().max()
print(max_reference_5)
print(min_reference_5)
if max_reference_5 > max :
  max = max_reference_5
  min = min_reference_5
print(max)
print(min)

testS1_pc6_s = pd.read_excel("PROBETA1_PASO_CARGA_6_VIGA_ROTA_sensor1_AUTOENCODER.xlsx")
min_reference_6 = testS1_pc6_s.min().min()
max_reference_6 = testS1_pc6_s.max().max()
print(max_reference_6)
print(min_reference_6)
if max_reference_6 > max :
  max = max_reference_6
  min = min_reference_6
print(max)
print(min)

"""## GRÁFICA DE LOS DATOS"""

plt.plot(sensor1_s['P_  1'], label = 'Baseline')
plt.plot(testS1_pc1_s['PP1_1'], label = 'Paso de carga 1')
plt.plot(testS1_pc2_s['PP2_1'], label = 'Paso de carga 2')
plt.plot(testS1_pc3_s['PP3_1'], label = 'Paso de carga 3')
plt.plot(testS1_pc4_s['PP4_1'], label = 'Paso de carga 4')
plt.plot(testS1_pc5_s['PP5_1'], label = 'Paso de carga 5')
plt.title('Impedances for Different Load Steps not including the state of rupture')
plt.legend()
plt.show()

"""## NORMALIZACIÓN DE LOS DATOS"""

sensor1 = (sensor1_s - min) / (max-min)

print(sensor1.head())

testS1_pc1_n = (testS1_pc1_s - min) / (max-min)
testS1_pc1_n2 = np.transpose(testS1_pc1_n)
testS1_pc1 = testS1_pc1_n2.values.reshape((testS1_pc1_n2.shape[0], testS1_pc1_n2.shape[1], 1))
print(testS1_pc1)
print(testS1_pc1.shape)

testS1_pc2_n = (testS1_pc2_s - min) / (max-min)
testS1_pc2_n2 = np.transpose(testS1_pc2_n)
testS1_pc2 = testS1_pc2_n2.values.reshape((testS1_pc2_n2.shape[0], testS1_pc2_n2.shape[1], 1))
print(testS1_pc2)
print(testS1_pc2.shape)

testS1_pc3_n = (testS1_pc3_s - min) / (max-min)
testS1_pc3_n2 = np.transpose(testS1_pc3_n)
testS1_pc3 = testS1_pc3_n2.values.reshape((testS1_pc3_n2.shape[0], testS1_pc3_n2.shape[1], 1))
print(testS1_pc3)
print(testS1_pc3.shape)

testS1_pc4_n = (testS1_pc4_s - min) / (max-min)
testS1_pc4_n2 = np.transpose(testS1_pc4_n)
testS1_pc4 = testS1_pc4_n2.values.reshape((testS1_pc4_n2.shape[0], testS1_pc4_n2.shape[1], 1))
print(testS1_pc4)
print(testS1_pc4.shape)

testS1_pc5_n = (testS1_pc5_s - min) / (max-min)
testS1_pc5_n2 = np.transpose(testS1_pc5_n)
testS1_pc5 = testS1_pc5_n2.values.reshape((testS1_pc5_n2.shape[0], testS1_pc5_n2.shape[1], 1))
print(testS1_pc5)
print(testS1_pc5.shape)

testS1_pc6_n = (testS1_pc6_s - min) / (max-min)
testS1_pc6_n2 = np.transpose(testS1_pc6_n)
testS1_pc6 = testS1_pc6_n2.values.reshape((testS1_pc6_n2.shape[0], testS1_pc6_n2.shape[1], 1))
print(testS1_pc6)
print(testS1_pc6.shape)

plt.plot(sensor1['P_  1'], label = 'Baseline')
plt.plot(testS1_pc1_n['PP1_1'], label = 'Paso de carga 1')
plt.plot(testS1_pc2_n['PP2_1'], label = 'Paso de carga 2')
plt.plot(testS1_pc3_n['PP3_1'], label = 'Paso de carga 3')
plt.plot(testS1_pc4_n['PP4_1'], label = 'Paso de carga 4')
plt.plot(testS1_pc5_n['PP5_1'], label = 'Paso de carga 5')
plt.title('Impedances normalized for Different Load Steps not including the state of rupture')
plt.legend()
plt.show()

"""## CREACIÓN DE LOS GRUPOS APRENDIZAJE Y TEST




"""

sensor1_t = np.transpose(sensor1)
print(sensor1_t.head())

from sklearn.model_selection import train_test_split
trainS1,testS1 = train_test_split(sensor1_t,train_size=0.75,random_state=111)
print(trainS1)

"""## PREPARACIÓN DATOS DE ENTRENAMIENTO


"""

trainS1 = trainS1.values.reshape((trainS1.shape[0], trainS1.shape[1], 1))
inputS1_shape = trainS1.shape[1:]
print(inputS1_shape)

trainS1.shape

trainS1.shape[0:2]

"""## RED NEURONAL

We will build a convolutional reconstruction autoencoder model. The model will
take input of shape `(batch_size, sequence_length, num_features)` and return
output of the same shape. In this case, `sequence_length` is 288 and
`num_features` is 1.
"""

modelS1 = Sequential()
modelS1.add(Conv1D(filters=64, kernel_size=5, activation='PReLU', padding='same',  kernel_regularizer=regularizers.l2(0.5), input_shape=inputS1_shape))
modelS1.add(Conv1D(filters=64, kernel_size=3, activation='PReLU', padding='same',kernel_regularizer=regularizers.l2(0.5)))
modelS1.add(Conv1D(filters=32, kernel_size=3, activation='PReLU', padding='same',kernel_regularizer=regularizers.l2(0.5)))
modelS1.add(Conv1D(filters=16, kernel_size=3, activation='PReLU', padding='same', kernel_regularizer=regularizers.l2(0.5)))
modelS1.add(Conv1D(filters=8, kernel_size=3, activation='PReLU', padding='same',kernel_regularizer=regularizers.l2(0.5)))

modelS1.add(Conv1DTranspose(filters=8, kernel_size=3, activation='PReLU', padding='same',kernel_regularizer=regularizers.l2(0.5)))
modelS1.add(Conv1DTranspose(filters=16, kernel_size=5, activation='PReLU', padding='same',kernel_regularizer=regularizers.l2(0.5)))
modelS1.add(Conv1DTranspose(filters=32, kernel_size=5, activation='PReLU', padding='same',kernel_regularizer=regularizers.l2(0.5)))
modelS1.add(Conv1DTranspose(filters=64, kernel_size=5, activation='PReLU', padding='same',kernel_regularizer=regularizers.l2(0.5)))
modelS1.add(Conv1DTranspose(filters=64, kernel_size=10, activation='PReLU', padding='same',kernel_regularizer=regularizers.l2(0.5)))

modelS1.add(Conv1D(filters=inputS1_shape[-1], kernel_size=3, activation='linear', padding='same'))

# Compilar el modelo
modelS1.compile(optimizer='adam', loss='mean_squared_error', metrics=['RootMeanSquaredError'])

trainS1_pred_SE = modelS1.predict(trainS1)
plt.plot(trainS1[0], label = 'Valor de la parte real de las impedancias de entrenamiento')
plt.plot(trainS1_pred_SE[0], label = 'Valor predecido de la parte real de las impedancias de entrenamiento')
plt.show()

"""## ENTRENAMIENTO DEL MODELO



"""

callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)
historyS1 = modelS1.fit(trainS1, trainS1, epochs=45, batch_size=5, validation_data=(testS1, testS1), callbacks = [callback])

"""Let's plot training and validation loss to see how the training went."""

plt.plot(historyS1.history["loss"], label="Training Loss")
plt.plot(historyS1.history["val_loss"], label="Validation Loss")
plt.legend()
plt.show()

optimizer = modelS1.optimizer
learning_rate = optimizer.learning_rate.numpy()
print("Tasa de aprendizaje:", learning_rate)

"""## DETECCIÓN DE ANOMALÍAS

We will detect anomalies by determining how well our model can reconstruct
the input data.


1.   Find RMSE loss on training samples.
2.   Find max RMSE loss value. This is the worst our model has performed trying
to reconstruct a sample. We will make this the `threshold` for anomaly
detection.
3.   If the reconstruction loss for a sample is greater than this `threshold`
value then we can infer that the model is seeing a pattern that it isn't
familiar with. We will label this sample as an `anomaly`.

"""

# Get train RMSE loss.
trainS1_pred = modelS1.predict(trainS1)
trainS1_pred2 = trainS1_pred.reshape((trainS1_pred.shape[0], trainS1_pred.shape[1]))
trainS1_2 = trainS1.reshape((trainS1.shape[0], trainS1.shape[1]))

trainS1_rmse_loss = rmse(trainS1_pred2,trainS1_2)

# plt.hist(trainS1_rmse_loss, bins=50)
# plt.xlabel("Train RMSE loss")
# plt.ylabel("No of samples")
# plt.show()

# # Get reconstruction loss threshold.
# thresholdS1 = np.max(trainS1_rmse_loss)
# print("Reconstruction error threshold: ", thresholdS1)

"""### Compare recontruction

Just for fun, let's see how our model has recontructed the first sample.
This is the 288 timesteps from day 1 of our training dataset.
"""

# Checking how the first sequence is learnt
plt.plot(trainS1[0])
plt.plot(trainS1_pred[0])
plt.show()

"""### Prepare test data"""

# fig, ax = plt.subplots()
# testS1.plot(legend=False, ax=ax)
# plt.show()
testS1 = testS1.values.reshape((testS1.shape[0], testS1.shape[1], 1))
# Get test RMSE loss.

testS1_pred = modelS1.predict(testS1)
testS1_pred2 =  testS1_pred.reshape((testS1_pred.shape[0], testS1_pred.shape[1]))
testS1_2 =  testS1.reshape((testS1.shape[0], testS1.shape[1]))
testS1_rmse_loss = rmse(testS1_pred2,testS1_2)


# plt.hist(testS1_rmse_loss, bins=50)
# plt.xlabel("test RMSE loss")
# plt.ylabel("No of samples")
# plt.show()

# # Detect all the samples which are anomalies.
# anomaliesS1 = testS1_rmse_loss > thresholdS1
# print("Number of anomaly samples: ", np.sum(anomaliesS1))
# print("Indices of anomaly samples: ", np.where(anomaliesS1))

plt.plot(testS1[0])
plt.plot(testS1_pred[0])
plt.show()

print(testS1_rmse_loss)

"""###PASO DE CARGA 1


"""

testS1_pc1_pred = modelS1.predict(testS1_pc1)
testS1_pc1_pred.shape

plt.plot(testS1_pc1[0], label='Original')
plt.plot(testS1_pc1_pred[0], label='Reconstructed')
plt.show()

testS1_pc1_pred2 =  testS1_pc1_pred.reshape((testS1_pc1_pred.shape[0], testS1_pc1_pred.shape[1]))
testS1_pc1_2 =  testS1_pc1.reshape((testS1_pc1.shape[0], testS1_pc1.shape[1]))
testS1_pc1_rmse_loss = rmse(testS1_pc1_pred2,testS1_pc1_2)



# plt.hist(testS1_pc1_rmse_loss, bins=50)
# plt.xlabel("test RMSE loss")
# plt.ylabel("No of samples")
# plt.show()

# # Detect all the samples which are anomalies.
# anomaliesS1_pc1 = testS1_pc1_rmse_loss > thresholdS1
# print("Number of anomaly samples: ", np.sum(anomaliesS1_pc1))
# print("Indices of anomaly samples: ", np.where(anomaliesS1_pc1))

print(testS1_pc1_rmse_loss)

# plt.scatter(1, trainS1_rmse_loss, label='Entrenamiento')
# plt.scatter(1, testS1_rmse_loss, label='Validación')
# plt.scatter(1, testS1_pc1_rmse_loss, label='Paso de carga 1')
# plt.axhline(y=thresholdS1, color='r', linestyle='--')
# plt.xlabel('Ensayos')
# plt.ylabel('RMSE')
# plt.legend()
# plt.show()

"""## PASO DE CARGA 1 CON VALORES ATÍPICOS"""

import random

testS1_pc1_n2_2 = testS1_pc1_n2.to_numpy() if isinstance(testS1_pc1_n2, pd.DataFrame) else testS1_pc1_n2
testS1_pc1_VA = testS1_pc1_n2_2.copy()
slice_to_modify = testS1_pc1_VA[:, 200:400]
random_values = np.random.randint(2, size=slice_to_modify.shape)
testS1_pc1_VA[:, 200:400] = random_values
print(testS1_pc1_VA)
plt.plot(testS1_pc1_VA.ravel()[0:7201], label='Paso de carga modificado')
plt.show()
print(testS1_pc1_VA.shape)

testS1_pc1_VA = testS1_pc1_VA.reshape((testS1_pc1_VA.shape[0], testS1_pc1_VA.shape[1], 1))
testS1_pc1_VA_pred = modelS1.predict(testS1_pc1_VA)
testS1_pc1_VA_pred.shape

plt.plot(testS1_pc1_VA[0], label='Paso de carga modificado')
plt.plot(testS1_pc1_VA_pred[0], label='Predicción del paso de carga modificado')
plt.legend()
plt.show()

"""###PASO DE CARGA 2"""

testS1_pc2_pred = modelS1.predict(testS1_pc2)
testS1_pc2_pred2 =  testS1_pc2_pred.reshape((testS1_pc2_pred.shape[0], testS1_pc2_pred.shape[1]))
testS1_pc2_2 =  testS1_pc2.reshape((testS1_pc2.shape[0], testS1_pc2.shape[1]))
testS1_pc2_rmse_loss = rmse(testS1_pc2_pred2,testS1_pc2_2)



# plt.hist(testS1_pc2_rmse_loss, bins=50)
# plt.xlabel("test RMSE loss")
# plt.ylabel("No of samples")
# plt.show()

# # Detect all the samples which are anomalies.
# anomaliesS1_pc2 = testS1_pc2_rmse_loss > thresholdS1
# print("Number of anomaly samples: ", np.sum(anomaliesS1_pc2))
# print("Indices of anomaly samples: ", np.where(anomaliesS1_pc2))

plt.plot(testS1_pc2[0], label='Original')
plt.plot(testS1_pc2_pred[0], label='Reconstructed')
plt.show()

# plt.scatter(1, trainS1_rmse_loss, label='Entrenamiento')
# plt.scatter(1, testS1_rmse_loss, label='Validación')
# plt.scatter(1, testS1_pc2_rmse_loss, label='Paso de carga 2')
# plt.axhline(y=thresholdS1, color='r', linestyle='--')
# plt.xlabel('Ensayos')
# plt.ylabel('RMSE')
# plt.legend()
# plt.show()

"""###PASO DE CARGA 3

"""

testS1_pc3_pred = modelS1.predict(testS1_pc3)
testS1_pc3_pred2 =  testS1_pc3_pred.reshape((testS1_pc3_pred.shape[0], testS1_pc3_pred.shape[1]))
testS1_pc3_2 =  testS1_pc3.reshape((testS1_pc3.shape[0], testS1_pc3.shape[1]))
testS1_pc3_rmse_loss = rmse(testS1_pc3_pred2, testS1_pc3_2)



# plt.hist(testS1_pc3_rmse_loss, bins=50)
# plt.xlabel("test RMSE loss")
# plt.ylabel("No of samples")
# plt.show()

# # Detect all the samples which are anomalies.
# anomaliesS1_pc3 = testS1_pc3_rmse_loss > thresholdS1
# print("Number of anomaly samples: ", np.sum(anomaliesS1_pc3))
# print("Indices of anomaly samples: ", np.where(anomaliesS1_pc3))

plt.plot(testS1_pc3[0], label='Original')
plt.plot(testS1_pc3_pred[0], label='Reconstructed')
plt.show()

# plt.scatter(1, trainS1_rmse_loss, label='Entrenamiento')
# plt.scatter(1, testS1_rmse_loss, label='Validación')
# plt.scatter(1, testS1_pc3_rmse_loss, label='Paso de carga 3')
# plt.axhline(y=thresholdS1, color='r', linestyle='--')
# plt.xlabel('Ensayos')
# plt.ylabel('RMSE')
# plt.legend()
# plt.show()

"""###PASO DE CARGA 4"""

testS1_pc4_pred = modelS1.predict(testS1_pc4)
testS1_pc4_pred2 =  testS1_pc4_pred.reshape((testS1_pc4_pred.shape[0], testS1_pc4_pred.shape[1]))
testS1_pc4_2 =  testS1_pc4.reshape((testS1_pc4.shape[0], testS1_pc4.shape[1]))
testS1_pc4_rmse_loss = rmse(testS1_pc4_pred2, testS1_pc4_2)



# plt.hist(testS1_pc4_rmse_loss, bins=50)
# plt.xlabel("test RMSE loss")
# plt.ylabel("No of samples")
# plt.show()

# # Detect all the samples which are anomalies.
# anomaliesS1_pc4 = testS1_pc4_rmse_loss > thresholdS1
# print("Number of anomaly samples: ", np.sum(anomaliesS1_pc4))
# print("Indices of anomaly samples: ", np.where(anomaliesS1_pc4))

plt.plot(testS1_pc4[0], label='Original')
plt.plot(testS1_pc4_pred[0], label='Reconstructed')
plt.show()

# plt.scatter(1, trainS1_rmse_loss, label='Entrenamiento')
# plt.scatter(1, testS1_rmse_loss, label='Validación')
# plt.scatter(1, testS1_pc4_rmse_loss, label='Paso de carga 4')
# plt.axhline(y=thresholdS1, color='r', linestyle='--')
# plt.xlabel('Ensayos')
# plt.ylabel('RMSE')
# plt.legend()
# plt.show()

"""###PASO DE CARGA 5"""

testS1_pc5_pred = modelS1.predict(testS1_pc5)
testS1_pc5_pred2 =  testS1_pc5_pred.reshape((testS1_pc5_pred.shape[0], testS1_pc5_pred.shape[1]))
testS1_pc5_2 =  testS1_pc5.reshape((testS1_pc5.shape[0], testS1_pc5.shape[1]))
testS1_pc5_rmse_loss = rmse(testS1_pc5_pred2,testS1_pc5_2)



# plt.hist(testS1_pc5_rmse_loss, bins=50)
# plt.xlabel("test RMSE loss")
# plt.ylabel("No of samples")
# plt.show()

# # Detect all the samples which are anomalies.
# anomaliesS1_pc5 = testS1_pc5_rmse_loss > thresholdS1
# print("Number of anomaly samples: ", np.sum(anomaliesS1_pc5))
# print("Indices of anomaly samples: ", np.where(anomaliesS1_pc5))

plt.plot(testS1_pc5[0], label='Original')
plt.plot(testS1_pc5_pred[0], label='Reconstructed')
plt.show()

# plt.scatter(1, trainS1_rmse_loss, label='Entrenamiento')
# plt.scatter(1, testS1_rmse_loss, label='Validación')
# plt.scatter(1, testS1_pc5_rmse_loss, label='Paso de carga 5')
# plt.axhline(y=thresholdS1, color='r', linestyle='--')
# plt.xlabel('Ensayos')
# plt.ylabel('RMSE')
# plt.legend()
# plt.show()

"""

###PASO DE CARGA 6"""

testS1_pc6_pred = modelS1.predict(testS1_pc6)
testS1_pc6_pred2 =  testS1_pc6_pred.reshape((testS1_pc6_pred.shape[0], testS1_pc6_pred.shape[1]))
testS1_pc6_2 =  testS1_pc6.reshape((testS1_pc6.shape[0], testS1_pc6.shape[1]))
testS1_pc6_rmse_loss = rmse(testS1_pc6_pred2, testS1_pc6_2)


# plt.hist(testS1_pc6_rmse_loss, bins=50)
# plt.xlabel("test RMSE loss")
# plt.ylabel("No of samples")
# plt.show()

# # Detect all the samples which are anomalies.
# anomaliesS1_pc6 = testS1_pc6_rmse_loss > thresholdS1
# print("Number of anomaly samples: ", np.sum(anomaliesS1_pc6))
# print("Indices of anomaly samples: ", np.where(anomaliesS1_pc6))

plt.plot(testS1_pc6[0], label='Original')
plt.plot(testS1_pc6_pred[0], label='Reconstructed')
plt.show()

# plt.scatter(1, trainS1_rmse_loss, label='Entrenamiento')
# plt.scatter(1, testS1_rmse_loss, label='Validación')
# plt.scatter(1, testS1_pc6_rmse_loss, label='Paso de carga 6')
# plt.axhline(y=thresholdS1, color='r', linestyle='--')
# plt.xlabel('Ensayos')
# plt.ylabel('RMSE')
# plt.legend()
# plt.show()

plt.bar(-1, trainS1_rmse_loss, label='Training')
plt.bar(0, testS1_rmse_loss, label='Validation')
plt.bar(1, testS1_pc1_rmse_loss, label='Paso de carga 1')
plt.bar(2, testS1_pc2_rmse_loss, label='Paso de carga 2')
plt.bar(3, testS1_pc3_rmse_loss, label='Paso de carga 3')
plt.bar(4, testS1_pc4_rmse_loss, label='Paso de carga 4')
plt.bar(5, testS1_pc5_rmse_loss, label='Paso de carga 5')
plt.xlabel('Load Steps')
plt.ylabel('RMSE')
plt.title('RMSE for Different Load Steps')
plt.legend()
plt.show()

plt.bar(-1, trainS1_rmse_loss, label='Training')
plt.bar(0, testS1_rmse_loss, label='Validation')
plt.bar(1, testS1_pc1_rmse_loss, label='Paso de carga 1')
plt.bar(2, testS1_pc2_rmse_loss, label='Paso de carga 2')
plt.bar(3, testS1_pc3_rmse_loss, label='Paso de carga 3')
plt.bar(4, testS1_pc4_rmse_loss, label='Paso de carga 4')
plt.bar(5, testS1_pc5_rmse_loss, label='Paso de carga 5')
plt.bar(6, testS1_pc6_rmse_loss, label='Paso de carga 6')
plt.xlabel('Load Steps')
plt.ylabel('RMSE')
plt.title('RMSE for Different Load Steps')
plt.legend()
plt.show()